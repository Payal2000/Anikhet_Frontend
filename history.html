<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Convolutional Neural Networks</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div id="tech-innovation">Technological <br> Innovation</div>
  
    <section>
        <nav><a href="index.html">Home</a></nav>
        <nav><a href="structure.html">Structure</a></nav>
        <nav><a href="history.html">History & more</a></nav>
        <nav><a href="uses.html">Uses & thoughts</a></nav>
        <nav><a href="references.html">References</a></nav>
        <a href="index.html" class="back-to-home-button">Back to Home</a>
    </section>

    <h1 id="history-h">Historical Evolution of CNN Architectures</h1>
    <div id="p3">
    <p>Kunihiko Fukushima and Yann LeCun were pioneers in the field of convolutional neural networks, starting with Fukushima's work in 1980 and LeCun's research on "Backpropagation Applied to Handwritten Zip Code Recognition" in 1989. Notably, LeCun utilized backpropagation to train neural networks to discern patterns in handwritten zip codes. His efforts throughout the 1990s led to the development of "LeNet-5", which applied these techniques to document recognition. Subsequent years saw the emergence of various CNN architectures,
        propelled by new datasets like MNIST and CIFAR-10, and contests such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Some notable architectures that followed include:
        <ol>
        <li>AlexNet</li>
        <li>VGGNet</li>
        <li>GoogLeNet</li>
        <li>ResNet</li>
        <li>ZFNet</li>
        </ol>
        Despite these developments, LeNet-5 remains recognized as the foundational CNN architecture.
        </p>
        </div>
       
        <h1 id="motivation-h">Motivation behind Convolution</h1>
        <div id="p4">
         <p>Convolution in computer vision capitalizes on three fundamental concepts: sparse interaction, parameter sharing, and equivariant representation. Let's delve deeper into each concept:

            In standard neural network layers, matrix multiplication uses a parameter matrix to dictate how input and output units interact. 
            Essentially, every output unit is influenced by every input unit. 
            In contrast, convolutional neural networks (CNNs) introduce sparse interaction. This is accomplished by using a kernel that's smaller than the input. 
            
            For instance, while an image may consist of thousands or even millions of pixels, a kernel can extract relevant details from just tens or hundreds of pixels. 
            
            As a result, fewer parameters are stored, which not only reduces the model's memory demands but also enhances its statistical performance.
            </p>
        </div>

            <footer>
                <p>Anikhet Mulky</p>
                <p><a href="mailto:hege@example.com">am9559@rit.edu</a></p>
              </footer>
</body>
</html>